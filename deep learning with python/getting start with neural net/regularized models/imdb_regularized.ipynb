{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"imdb_regularized.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Ef4bbrWNFRL6","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.datasets import imdb\n","from keras.models import Sequential\n","from keras.layers import Dense,Dropout\n","from keras import regularizers\n","\n","from sklearn.model_selection import train_test_split\n","\n","import numpy as np\n","import matplotlib as plt\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZJ3x9MCtcbP0","colab_type":"code","colab":{}},"cell_type":"code","source":["num_features = 10000\n","(train_data, train_targets), (test_data, test_targets) = imdb.load_data(num_words=num_features)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4-KCuVrAcoMh","colab_type":"code","colab":{}},"cell_type":"code","source":["def vectorizer(sequences, dimension=10000):\n","  data = np.zeros(shape=(len(sequences),dimension))\n","  for i, sequence in enumerate(sequences):\n","    data[i, sequence] = 1\n","  return data\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"mfvv5LOOg4lj","colab_type":"code","colab":{}},"cell_type":"code","source":["train_data = vectorizer(train_data)\n","test_data = vectorizer(test_data)\n","#insted of vectorizer keras has an in-built function called tokenizer\n","#from keras.preprocessing.text import Tokenizer\n","#tokenizer = Tokenizer(num_words=number_of_features)\n","#train_features = tokenizer.sequences_to_matrix(train_data, mode='binary')\n","#test_features = tokenizer.sequences_to_matrix(test_data, mode='binary')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vWBJ5c7whNEJ","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train, X_val, y_train, y_val = train_test_split(train_data, train_targets, test_size=0.25,random_state=0,stratify=train_targets) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"4ecyIrgvkaCx","colab_type":"code","colab":{}},"cell_type":"code","source":["def build_model(neuron):\n","  model = Sequential()\n","  model.add(Dense(neuron, activation=\"relu\"),input_shape=(num_features,))\n","  model.add(Dense(neuron, activation=\"relu\"))\n","  model.add(Dense(1,activation=\"sigmoid\"))\n","  model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=[\"acc\"])\n","  return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MCiMV6Ell1WM","colab_type":"text"},"cell_type":"markdown","source":["<p><font color=\"green\">Here is a universal blueprint to attack and solve any machine learning problem</font></p>\n","<p>1) Define the problem and assembling a dataset</p>\n","<p>2) Choosing a measure of success</p>\n","<p>3) Choosing an evaluation protocol</p>\n","<p>4) prepare the data before feeding it into a neural network</p>\n","<p>5) Developping a moel that does better than a baseline</p>\n","<p>6) Developping a model that overfits</p>\n","<p>7) Regularizing the model (hyperparameter tunning)</p>"]},{"metadata":{"id":"sbleyZkRm6JA","colab_type":"code","colab":{}},"cell_type":"code","source":["model1 = build_model(16)\n","history1 = model1.fit(X_train,y_train,epochs=20, batch_size=512,validation_data=(X_val, y_val))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JT8_JMoeK2fK","colab_type":"text"},"cell_type":"markdown","source":["<h3><center><font color=\"red\">Analysing our model : we can use tensorboard colab of course, but let's do it manually</font></center></h3>"]},{"metadata":{"id":"QyqhV_BoLJIn","colab_type":"code","colab":{}},"cell_type":"code","source":["hist1 = history1.history\n","loss = hist1[\"loss\"]\n","val_loss = hist2[\"val_loss\"]\n","epochs = range(1,len(hist1)+1)\n","plt.plot(epochs,loss, \"b\",legend=\"Training loss\")\n","plt.plot(epochs,loss, \"b\",legend=\"Validation loss\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"Validation and training loss\")\n","plt.legend()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iJ-eBesTGnSi","colab_type":"text"},"cell_type":"markdown","source":["<h3><center><font color=\"red\">Tackling ovefittting using a small network</font></center></h3>"]},{"metadata":{"id":"8_bOQ9CKGzuQ","colab_type":"code","colab":{}},"cell_type":"code","source":["model2 = build_model(4)\n","history2 = model2.fit(X_train,y_train,epochs=20, batch_size=512,validation_data=(X_val, y_val))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XtVCB3FoHp11","colab_type":"code","colab":{}},"cell_type":"code","source":["hist1 = history1.history\n","hist2 = history2.history\n","val_loss_1 = hist1[\"val_loss\"]\n","val_loss_2 = hist2[\"val_loss\"]\n","epochs = range(1,len(hist1)+1)\n","\n","plt.plot(epochs,val_loss_1,'b',label=\"original model\")\n","plt.plot(epochs,val_loss_2,'o',label=\"smaller model\")\n","plt.title(\"Validation losses\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss values\")\n","plt.legend()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h6Zqax7biv5v","colab_type":"text"},"cell_type":"markdown","source":["<h3><center><font color=\"red\">using weight decay (L2 regularization)</font></center></h3>"]},{"metadata":{"id":"Yg6XGKo1i-LD","colab_type":"code","colab":{}},"cell_type":"code","source":["model3 = Sequential()\n","model3.add(Dense(16,activation=\"relu\",kernel_regularizer=regularizers.l2(0.001),input_shape=(num_features,)))\n","model3.add(Dense(16,activation=\"relu\",kernel_regularizer=regularizers.l2(0.001)))\n","model3.add(Dense(1,activation=\"sigmoid\"))\n","model3.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n","model3.fit(X_train,y_train,epochs=20, batch_size=512,validation_data=(X_val, y_val))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"18oNiwP52NpI","colab_type":"code","colab":{}},"cell_type":"code","source":["hist3 = history2.history\n","val_loss_3 = hist3[\"val_loss\"]\n","\n","plt.plot(epochs,val_loss_1,'b',label=\"original model\")\n","plt.plot(epochs,val_loss_3,'o',label=\"model with l2 regul.\")\n","plt.title(\"Validation losses\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss values\")\n","plt.legend()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4DoFRAGc2D78","colab_type":"text"},"cell_type":"markdown","source":["<h3><center><font color=\"red\">using dropout dropout</font></center></h3>"]},{"metadata":{"id":"fPAzNVAJ2Hyg","colab_type":"code","colab":{}},"cell_type":"code","source":["model4 = Sequential()\n","model4.add(Dense(16,activation=\"relu\",input_shape=(num_features,)))\n","model4.add(Dropout(0.3))\n","model4.add(Dense(16,activation=\"relu\"))\n","model4.add(Dropout(0.3))\n","model4.add(Dense(1,activation=\"sigmoid\"))\n","model4.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n","model4.fit(X_train,y_train,epochs=20, batch_size=512,validation_data=(X_val, y_val))"],"execution_count":0,"outputs":[]}]}